{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api key found, good to go\n"
     ]
    }
   ],
   "source": [
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if API_KEY and API_KEY.startswith(\"sk-proj-\") and len(API_KEY)>10:\n",
    "    print(\"api key found, good to go\")\n",
    "else:\n",
    "    print(\"api key not found, please set the OPENAPI-API_KEY environment variable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "MODEL_GPT =\"gpt-4o-mini\"\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt=\"You are a senior developer guiding junior developer. Clear and concise answers, chop to smaller steps if topic is complicated.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    " \n",
    "question = input(\"Present thy queries, thou haughty mortal.\")\n",
    "# print(f\"Thy question was: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question, MODEL, stream_answer=False):\n",
    "  response = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": sys_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    stream=stream_answer\n",
    "  )\n",
    "  \n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_answer(response):\n",
    "  for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "# Set up the OpenAI API client\n",
    "# openai = OpenAI()\n",
    "\n",
    "# response = get_response(question, MODEL_GPT, True)\n",
    "# stream_answer(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split llama3.2 response into smaller chunks\n",
    "def split_response(response):\n",
    "    chunks = response.split('\\n')\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Splitting Long Strings into Displayable Chunks**\n",
      "\n",
      "Suppose you have a long string that needs to be displayed in chunks for user interface purposes. You want to identify the optimal length of each chunk based on your screen resolution and font size.\n",
      "\n",
      "Here's a step-by-step solution:\n",
      "\n",
      "### 1. Determine the screen width and font size\n",
      "\n",
      "Identify the available screen width and font size used in your application. These values will serve as the reference for calculating the ideal chunk length.\n",
      "\n",
      "```javascript\n",
      "// Assume screen width is 500 pixels\n",
      "const screenWidth = 500;\n",
      "\n",
      "// Assume font size is 16 pixels (Arial by default)\n",
      "const fontSize = 16;\n",
      "```\n",
      "\n",
      "### 2. Calculate the maximum character width\n",
      "\n",
      "Calculate the approximate average character width based on your font size.\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Average character width in pixels for a given font size.\n",
      " * @param {number} fontSize - Font size in pixels.\n",
      " * @returns {number}\n",
      " */\n",
      "function calculateCharWidth(fontSize) {\n",
      "  // Assuming average character spacing is about 0.5em (50px)\n",
      "  return fontSize / 10;\n",
      "}\n",
      "\n",
      "const charWidth = calculateCharWidth(fontSize);\n",
      "```\n",
      "\n",
      "### 3. Determine the optimal chunk length\n",
      "\n",
      "Calculate the maximum possible chunk length based on your screen width and font size.\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Calculates the maximum possible chunk length.\n",
      " * @param {number} screenWidth - Screen width in pixels.\n",
      " * @param {number} charWidth - Average character width in pixels.\n",
      " * @returns {number}\n",
      " */\n",
      "function calculateChunkLength(screenWidth, charWidth) {\n",
      "  // A common convention is to keep the text wrap around 70-80 characters\n",
      "  return Math.min(Math.floor((screenWidth / (charWidth + 10)) * 0.8), 100);\n",
      "}\n",
      "\n",
      "const chunkLength = calculateChunkLength(screenWidth, charWidth);\n",
      "```\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "```javascript\n",
      "// Original long string\n",
      "const longString = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\";\n",
      "\n",
      "// Split the string into displayable chunks\n",
      "const chunks = [];\n",
      "let currentChunk = \"\";\n",
      "for (let i = 0; i < longString.length; ) {\n",
      "  // Consider adding more logic if you expect variable widths for different lines.\n",
      "  const substringIndex = longString.substring(i).indexOf(\" \", chunkLength - 1);\n",
      "  let endIndex;\n",
      "  if (substringIndex === -1) {\n",
      "    endIndex = longString.indexOf(\"\\n\", i);\n",
      "    break;\n",
      "  } else {\n",
      "    endIndex = i + substringIndex;\n",
      "  }\n",
      "\n",
      "  // Determine whether to split the current substring\n",
      "  if (i > 0 && endIndex < longString.length) {\n",
      "    const chunk = longString.substring(i, endIndex).trim();\n",
      "    chunks.push(...chunk.split(\" \"));\n",
      "  }\n",
      "  chunkLength -= 10; // Allow for some padding and a line break\n",
      "\n",
      "  i = endIndex + 1;\n",
      "}\n",
      "\n",
      "console.log(chunks);\n",
      "// Output:\n",
      "// [ \"Lorem ipsum\", \"dolor\", \"sit amet,\", \"consectetur adipiscing\", \"elit\" ]\n",
      "```\n",
      "\n",
      "Note that you might need to tune the optimization functions further, such as considering different font styles and varying character widths for different characters.\n"
     ]
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "# Change openai to use the local Llama 3.2 model\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "\n",
    "response = get_response(question, MODEL_LLAMA)\n",
    "for chunk in split_response(response.choices[0].message.content):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
